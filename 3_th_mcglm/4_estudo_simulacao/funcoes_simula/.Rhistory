m3 = list(p = p, size = 1)
)
}
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
}
sample_size = 50
n_treatment = 4
betas = c(0.5,0,0,0)
n_datasets = 500
n_distances = 20
distribution = 'binomial'
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
## Marginais
switch(distribution,
"poisson" = {
invcdfnames <- c("qpois","qpois", "qpois")
link <- "log"
variance <- "tweedie"
},
"binomial" = {
invcdfnames <- c("qbinom","qbinom", "qbinom")
link <- "logit"
variance <- "binomialP"
}
)
cor_matrix <- matrix(c(1.0,  0.75, 0.5,
0.75,  1.0, 0.25,
0.5,  0.25, 1.0
),3,3)
# lista para armazenar os conjuntos de dados
datasets <- list()
for (i in 1:(n_datasets+15)) {
# Argumentos das marginais
switch(distribution,
"poisson" = {
lambda <- exp(X%*%betas)
paramslists <- list(
m1 = list(lambda = lambda),
m2 = list(lambda = lambda),
m3 = list(lambda = lambda)
)
},
"binomial" = {
p <- exp(X%*%betas)/(1 + exp(X%*%betas))
paramslists <- list(
m1 = list(p = p, size = 1),
m2 = list(p = p, size = 1),
m3 = list(p = p, size = 1)
)
}
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
}
datasets
form1 = y1~x
form2 = y2~x
form3 = y3~x
Z0 <- mc_id(datasets[[1]])
models <- list()
sample_size < 100
models <- list()
switch(distribution,
"poisson" = {
for (i in 1:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
data = datasets[[i]])
models[[i]] <- fit
print(i)
}
},
"binomial" = {
if (sample_size < 100) {
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 0.5)
} else {
ca <- list()
}
for (i in 1:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
Ntrial = list(1,1,1),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
}
)
sample_size = 50
n_treatment = 4
betas = c(0.5,0,0,0)
n_datasets = 500
n_distances = 20
distribution = 'binomial'
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
## Marginais
switch(distribution,
"poisson" = {
invcdfnames <- c("qpois","qpois", "qpois")
link <- "log"
variance <- "tweedie"
},
"binomial" = {
invcdfnames <- c("qbinom","qbinom", "qbinom")
link <- "logit"
variance <- "binomialP"
}
)
cor_matrix <- matrix(c(1.0,  0.75, 0.5,
0.75,  1.0, 0.25,
0.5,  0.25, 1.0
),3,3)
# lista para armazenar os conjuntos de dados
datasets <- list()
for (i in 1:(n_datasets+15)) {
# Argumentos das marginais
switch(distribution,
"poisson" = {
lambda <- exp(X%*%betas)
paramslists <- list(
m1 = list(lambda = lambda),
m2 = list(lambda = lambda),
m3 = list(lambda = lambda)
)
},
"binomial" = {
p <- exp(X%*%betas)/(1 + exp(X%*%betas))
paramslists <- list(
m1 = list(p = p, size = 1),
m2 = list(p = p, size = 1),
m3 = list(p = p, size = 1)
)
}
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
}
form1 = y1~x
form2 = y2~x
form3 = y3~x
Z0 <- mc_id(datasets[[1]])
models <- list()
switch(distribution,
"poisson" = {
for (i in 1:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
data = datasets[[i]])
models[[i]] <- fit
print(i)
}
},
"binomial" = {
if (sample_size < 100) {
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 0.5)
} else {
ca <- list()
}
for (i in 1:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
Ntrial = list(1,1,1),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
}
)
i=325
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 0.5)
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
Ntrial = list(1),
data = datasets[[i]],
control_algorithm = ca)
i=325
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 0.5)
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
Ntrial = list(1,1,1),
data = datasets[[i]],
control_algorithm = ca)
i=325
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 1)
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
Ntrial = list(1,1,1),
data = datasets[[i]],
control_algorithm = ca)
for (i in 326:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link, link, link),
variance = c(variance, variance, variance),
Ntrial = list(1,1,1),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
dists <- vector() # vetor para armazenar as distancias
dists[1] <- 0 # distancia inicial 0
hyp_betas <- betas # vetor inicial para distribuir os efeitos
hypothesis <- list() # vetor para armazenar as hipoteses
# hipotese inicial
hypothesis[[1]] <- paste(coef(models[[1]], type = 'beta')$Parameters,
'=',
rep(betas, 3))
# obtenção das distâncias e hipóteses a serem testadas
for (i in 2:n_distances) {
hyp_betas[1] <- hyp_betas[1] - (betas[1]/n_distances)
hyp_betas[2:length(betas)] <- hyp_betas[2:length(betas)] + (betas[1]/n_distances)/(n_treatment-1)
hypothesis[[i]] <- paste(coef(models[[1]], type = 'beta')$Parameters,
'=',
rep(hyp_betas, 3))
dists[[i]] <- dist(rbind(betas, hyp_betas), method = "euclidean")
}
dists <- dists/sd(dists)
dists
parameters <- data.frame(Parameters = coef(models[[1]])$Parameters,
Type = coef(models[[1]])$Type)
parameters
for (i in 1:(n_datasets+15)) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
vcovs <- list()
for (i in 1:(n_datasets+15)) {
vcovs[[i]] <- vcov(models[[i]])
}
parameters
p_test <- matrix(nrow = length(hypothesis),
ncol = length(models))
for (i in 1:length(models)) {
for (j in 1:length(hypothesis)) {
p_test[j,i] <- try(mc_linear_hypothesis(object =  models[[i]],
hypothesis = hypothesis[[j]])$P_valor)
}
}
#----------------------------------------------------------------
# minhas funções
source('~/msc/3_th_mcglm/0_funcoes/functions.R')
p_test <- matrix(nrow = length(hypothesis),
ncol = length(models))
for (i in 1:length(models)) {
for (j in 1:length(hypothesis)) {
p_test[j,i] <- try(mc_linear_hypothesis(object =  models[[i]],
hypothesis = hypothesis[[j]])$P_valor)
}
}
# converte resultado para dataframe
p_test <- as.data.frame(p_test)
# caso tenha falhado, converte para NA
for (i in 1:ncol(p_test)) {
p_test[,i] <- as.numeric(p_test[,i])
}
rej <- ifelse(p_test[,1:(ncol(p_test))] < 0.05, 1, 0)
index_problems <- names(which(colSums(is.na(p_test)) > 0))
rej2 <- rej[,!(colnames(rej) %in% index_problems)][,1:n_datasets]
df_final <- data.frame(dist = dists,
rej = ((rowSums(rej2))/ncol(rej2))*100)
df_final
df_final$distribution <- paste('tri', distribution)
df_final$sample_size <- sample_size
#df_final$n_datasets <- (ncol(p_test[ , colSums(is.na(p_test)) == 0])-1)
df_final$n_datasets <- ncol(rej2)
# retorna dataframe com o percentual de rejeição para cada hipótese
return(list(hypothesis = hypothesis,
parameters = parameters,
vcovs = vcovs,
p_test = p_test,
index_problems = index_problems,
df_final = df_final))
df_final
sample_size = 50
n_treatment = 4
betas_normal = c(5,0,0,0)
betas_poisson = c(2.3,0,0,0)
betas_binomial = c(0.5,0,0,0)
n_datasets = 500
n_distances = 20
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
invcdfnames <- c("qnorm","qpois", "qbinom")
cor_matrix <- matrix(c(1.0,  0.75, 0.5,
0.75,  1.0, 0.25,
0.5,  0.25, 1.0
),3,3)
# lista para armazenar os conjuntos de dados
datasets <- list()
for (i in 1:n_datasets) {
# Argumentos das marginais
mu <- X%*%betas_normal
lambda <- exp(X%*%betas_poisson)
p <- exp(X%*%betas_binomial)/(1 + exp(X%*%betas_binomial))
paramslists <- list(
m1 = list(mean = mu, sd = 1),
m2 = list(lambda = lambda),
m3 = list(p = p, size = 1)
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
}
datasets
for (i in 1:99) {
# Argumentos das marginais
mu <- X%*%betas_normal
lambda <- exp(X%*%betas_poisson)
p <- exp(X%*%betas_binomial)/(1 + exp(X%*%betas_binomial))
paramslists <- list(
m1 = list(mean = mu, sd = 1),
m2 = list(lambda = lambda),
m3 = list(p = p, size = 1)
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
}
datasets
form1 = y1~x
form2 = y2~x
form3 = y3~x
link_normal <- "identity"
variance_normal <- "constant"
link_poisson <- "log"
variance_poisson <- "tweedie"
link_binomial <- "logit"
variance_binomial <- "binomialP"
Z0 <- mc_id(datasets[[1]])
models <- list()
if (sample_size < 100) {
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 0.5)
} else {
ca <- list()
}
ca
sample_size
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link_normal, link_poisson, link_binomial),
variance = c(variance_normal, variance_poisson, variance_binomial),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
for (i in 1:99) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link_normal, link_poisson, link_binomial),
variance = c(variance_normal, variance_poisson, variance_binomial),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
models
dists <- vector() # vetor para armazenar as distancias
dists[1] <- 0 # distancia inicial 0
hyp_betas_normal <- betas_normal # vetor inicial para distribuir os efeitos
hyp_betas_poisson <- betas_poisson # vetor inicial para distribuir os efeitos
hyp_betas_binomial <- betas_binomial # vetor inicial para distribuir os efeitos
hypothesis <- list() # vetor para armazenar as hipoteses
# hipotese inicial
hypothesis[[1]] <- paste(coef(models[[1]], type = 'beta')$Parameters,
'=',
c(betas_normal,
betas_poisson,
betas_binomial))
# obtenção das distâncias e hipóteses a serem testadas
for (i in 2:n_distances) {
hyp_betas_normal[1] <- hyp_betas_normal[1] - (betas_normal[1]/n_distances)
hyp_betas_normal[2:length(betas_normal)] <- hyp_betas_normal[2:length(betas_normal)] + (betas_normal[1]/n_distances)/(n_treatment-1)
hyp_betas_poisson[1] <- hyp_betas_poisson[1] - (betas_poisson[1]/n_distances)
hyp_betas_poisson[2:length(betas_poisson)] <- hyp_betas_poisson[2:length(betas_poisson)] + (betas_poisson[1]/n_distances)/(n_treatment-1)
hyp_betas_binomial[1] <- hyp_betas_binomial[1] - (betas_binomial[1]/n_distances)
hyp_betas_binomial[2:length(betas_binomial)] <- hyp_betas_binomial[2:length(betas_binomial)] + (betas_binomial[1]/n_distances)/(n_treatment-1)
hypothesis[[i]] <- paste(coef(models[[1]], type = 'beta')$Parameters,
'=',
c(hyp_betas_normal,
hyp_betas_poisson,
hyp_betas_binomial))
dists[[i]] <- dist(rbind(betas_normal,
hyp_betas_normal),
method = "euclidean")
}
dists <- dists/sd(dists)
parameters <- data.frame(Parameters = coef(models[[1]])$Parameters,
Type = coef(models[[1]])$Type)
for (i in 1:99) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
for (i in 1:99) {
vcovs[[i]] <- vcov(models[[i]])
}
vcovs <- list()
for (i in 1:99) {
vcovs[[i]] <- vcov(models[[i]])
}
p_test <- matrix(nrow = length(hypothesis),
ncol = length(models))
for (i in 1:length(models)) {
for (j in 1:length(hypothesis)) {
p_test[j,i] <- try(mc_linear_hypothesis(object =  models[[i]],
hypothesis = hypothesis[[j]])$P_valor)
}
}
#----------------------------------------------------------------
# minhas funções
source('~/msc/3_th_mcglm/0_funcoes/functions.R')
p_test <- matrix(nrow = length(hypothesis),
ncol = length(models))
for (i in 1:length(models)) {
for (j in 1:length(hypothesis)) {
p_test[j,i] <- try(mc_linear_hypothesis(object =  models[[i]],
hypothesis = hypothesis[[j]])$P_valor)
}
}
# converte resultado para dataframe
p_test <- as.data.frame(p_test)
# caso tenha falhado, converte para NA
for (i in 1:ncol(p_test)) {
p_test[,i] <- as.numeric(p_test[,i])
}
# acrescenta info de distancia
p_test$dist <- dists
rej <- ifelse(p_test[,1:(ncol(p_test)-1)] < 0.05, 1, 0)
df_final <- data.frame(dist = p_test$dist,
rej = (rowSums(rej, na.rm = T)/
(ncol(p_test[ , colSums(is.na(p_test)) == 0])-1)*100))
df_final$distribution <- 'normal/poisson/binomial'
df_final$sample_size <- sample_size
df_final$n_datasets <- (ncol(p_test[ , colSums(is.na(p_test)) == 0])-1)
df_final
