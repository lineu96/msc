x = trat)
}
}
}
)
th_mcglm_sim2 <- function(sample_size = 25,
n_datasets = 10,
n_treatment = 4,
betas_iniciais = c(5,0,0,0),
n_distances = 20,
distribution = 'normal')
{
betas <- list() # lista para armazenar os betas para gerar datasets
betas[[1]] <- betas_iniciais # primeiro elemento igual aos betas iniciais
dists <- vector() # vetor para armazenar as distancias
dists[1] <- 0 # distancia inicial 0
hyp_betas <- betas_iniciais # valor inicial para distribuição de efeitos
#----------------------------------------------------------------
# obtenção dos betas para simular
for (i in 2:n_distances) {
hyp_betas[1] <- hyp_betas[1] - (betas_iniciais[1]/n_distances)
hyp_betas[2:length(betas_iniciais)] <- hyp_betas[2:length(betas_iniciais)] + (betas_iniciais[1]/n_distances)/(n_treatment-1)
betas[[i]] <- hyp_betas
dists[[i]] <- dist(rbind(betas_iniciais, hyp_betas), method = "euclidean")
}
#----------------------------------------------------------------
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
# lista para armazenar os conjuntos de dados
datasets <- list()
# geração dos conjuntos de dados
# switch para simular de diferentes distribuições
# (normal, poisson, binomial n=10 ou beta)
# e definir função de ligação e variância para
# ajuste dos modelos (depende da distribuição)
switch(distribution,
"normal" = {
link <- c("identity")
variance <- c("constant")
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
mu <- X%*%betas[[j]]
y <- rnorm(sample_size, mean = mu, sd = 1)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"poisson" = {
link <- c("log")
variance <- c("tweedie")
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
lambda <- exp(X%*%betas[[j]])
y <- rpois(sample_size,
lambda = lambda)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"binomial" = {
link <-  "logit"
variance  <-  "binomialP"
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
p <- exp(X%*%betas[[j]])/(1 + exp(X%*%betas[[j]]))
y <- rbinom(sample_size,
p = p,
size = 10)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"beta" = {
link <- "logit"
variance <- "binomialP"
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
p <- exp(X%*%betas[[j]])/(1 + exp(X%*%betas[[j]]))
y <- gamlss.dist::rBE(sample_size,
mu = p,
sigma = 0.2)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
}
)
#----------------------------------------------------------------
# elementos mcglm
# caso seja binomial a resposta precisa ser declarada como razão
# y/Ntrial ~x
switch(distribution,
"binomial" = {form = y/10~x},
{form = y~x}
)
# preditor matricial
Z0 <- mc_id(datasets[[1]][[1]]) # matriz identidade para o preditor matricial
#----------------------------------------------------------------
models <- list()
switch(distribution,
"binomial" = {
for (j in 1:length(betas)) {
models[[j]] <- list()
length(models[[j]]) <- n_datasets
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
Ntrial = list(10),
data = datasets[[j]][[i]])
models[[j]][[i]] <- fit
print(paste(j,i))
}
}
},
{
for (j in 1:length(betas)) {
models[[j]] <- list()
length(models[[j]]) <- n_datasets
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
data = datasets[[j]][[i]])
models[[j]][[i]] <- fit
print(paste(j,i))
}
}
}
)
#----------------------------------------------------------------
hypothesis <- paste(coef(models[[1]][[1]], type = 'beta')$Parameters,
'=',
betas_iniciais)
#----------------------------------------------------------------
p_test <- matrix(nrow = length(betas),
ncol = n_datasets)
for (j in 1:length(betas)) {
for (i in 1:n_datasets) {
p_test[j,i] <-  mc_linear_hypothesis(object =  models[[j]][[i]],
hypothesis = hypothesis)$P_valor
}
}
#----------------------------------------------------------------
# converte resultado para dataframe
p_test <- as.data.frame(p_test)
#----------------------------------------------------------------
# acrescenta info de distancia
p_test$dist <- dists
#----------------------------------------------------------------
# obtém percentual de rejeição
rej <- ifelse(p_test[,1:(ncol(p_test)-1)] < 0.05, 1, 0)
df_final <- data.frame(dist = p_test$dist,
rej = (rowSums(rej)/(ncol(p_test)-1)*100))
#----------------------------------------------------------------
# retorna dataframe com o percentual de rejeição para cada hipótese
return(df_final)
}
source('~/msc/3_th_mcglm/4_estudo_simulacao/libs.R')
source('~/msc/3_th_mcglm/4_estudo_simulacao/th_mcglm_sim.R')
source('~/msc/3_th_mcglm/4_estudo_simulacao/analises/grafico.R')
sample_size = 100
n_datasets = 2
n_treatment = 4
betas_iniciais = c(5,0,0,0)
n_distances = 5
normal <- th_mcglm_sim2(sample_size = sample_size,
n_datasets = n_datasets,
n_treatment = n_treatment,
betas_iniciais = betas_iniciais,
n_distances = n_distances,
distribution = 'normal')
poisson <- th_mcglm_sim2(sample_size = sample_size,
n_datasets = n_datasets,
n_treatment = n_treatment,
betas_iniciais = betas_iniciais,
n_distances = n_distances,
distribution = 'poisson')
binomial <- th_mcglm_sim2(sample_size = sample_size,
n_datasets = n_datasets,
n_treatment = n_treatment,
betas_iniciais = betas_iniciais,
n_distances = n_distances,
distribution = 'binomial')
beta <- th_mcglm_sim2(sample_size = sample_size,
n_datasets = n_datasets,
n_treatment = n_treatment,
betas_iniciais = betas_iniciais,
n_distances = n_distances,
distribution = 'beta')
x11()
par(mfrow=c(2,2),oma = c(0, 0, 2, 0))
grafico(normal_n50, main = 'Normal')
grafico(normal, main = 'Normal')
grafico(poisson, main = 'Poisson')
grafico(binomial, main = 'Binomial')
grafico(beta, main = 'Beta')
sample_size = 100
n_datasets = 100
n_treatment = 4
betas_iniciais = c(5,0,0,0)
n_distances = 20
normal <- th_mcglm_sim2(sample_size = sample_size,
n_datasets = n_datasets,
n_treatment = n_treatment,
betas_iniciais = betas_iniciais,
n_distances = n_distances,
distribution = 'normal')
normal
grafico(normal, main = 'Normal')
sample_size = 100
n_datasets = 100
n_treatment = 4
betas_normal = c(5,0,0,0)
n_distances = 20
normal2 <- th_mcglm_sim(sample_size = sample_size,
n_datasets = n_datasets,
n_treatment = n_treatment,
betas = betas_normal,
n_distances = n_distances,
distribution = 'normal')
grafico(normal2, main = 'Normal2')
distribution = 'binomial'
sample_size = 100
n_datasets = 2
n_treatment = 4
betas_iniciais = c(5,0,0,0)
n_distances = 5
betas <- list() # lista para armazenar os betas para gerar datasets
betas[[1]] <- betas_iniciais # primeiro elemento igual aos betas iniciais
dists <- vector() # vetor para armazenar as distancias
dists[1] <- 0 # distancia inicial 0
hyp_betas <- betas_iniciais # valor inicial para distribuição de efeitos
for (i in 2:n_distances) {
hyp_betas[1] <- hyp_betas[1] - (betas_iniciais[1]/n_distances)
hyp_betas[2:length(betas_iniciais)] <- hyp_betas[2:length(betas_iniciais)] + (betas_iniciais[1]/n_distances)/(n_treatment-1)
betas[[i]] <- hyp_betas
dists[[i]] <- dist(rbind(betas_iniciais, hyp_betas), method = "euclidean")
}
betas
dists
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
# lista para armazenar os conjuntos de dados
datasets <- list()
switch(distribution,
"normal" = {
link <- c("identity")
variance <- c("constant")
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
mu <- X%*%betas[[j]]
y <- rnorm(sample_size, mean = mu, sd = 1)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"poisson" = {
link <- c("log")
variance <- c("tweedie")
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
lambda <- exp(X%*%betas[[j]])
y <- rpois(sample_size,
lambda = lambda)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"binomial" = {
link <-  "logit"
variance  <-  "binomialP"
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
p <- exp(X%*%betas[[j]])/(1 + exp(X%*%betas[[j]]))
y <- rbinom(sample_size,
p = p,
size = 10)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"beta" = {
link <- "logit"
variance <- "binomialP"
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
p <- exp(X%*%betas[[j]])/(1 + exp(X%*%betas[[j]]))
y <- gamlss.dist::rBE(sample_size,
mu = p,
sigma = 0.2)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
}
)
switch(distribution,
"binomial" = {form = y/10~x},
{form = y~x}
)
# preditor matricial
Z0 <- mc_id(datasets[[1]][[1]]) # matriz identidade para o preditor matricial
models <- list()
switch(distribution,
"binomial" = {
for (j in 1:length(betas)) {
models[[j]] <- list()
length(models[[j]]) <- n_datasets
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
Ntrial = list(10),
data = datasets[[j]][[i]])
models[[j]][[i]] <- fit
print(paste(j,i))
}
}
},
{
for (j in 1:length(betas)) {
models[[j]] <- list()
length(models[[j]]) <- n_datasets
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
data = datasets[[j]][[i]])
models[[j]][[i]] <- fit
print(paste(j,i))
}
}
}
)
betas[[1]]
models[[1]]
betas[[2]]
models[[2]]
betas[[3]]
models[[3]]
betas[[4]]
models[[4]]
betas[[5]]
models[[5]]
distribution = 'beta'
betas <- list() # lista para armazenar os betas para gerar datasets
betas[[1]] <- betas_iniciais # primeiro elemento igual aos betas iniciais
dists <- vector() # vetor para armazenar as distancias
dists[1] <- 0 # distancia inicial 0
hyp_betas <- betas_iniciais # valor inicial para distribuição de efeitos
for (i in 2:n_distances) {
hyp_betas[1] <- hyp_betas[1] - (betas_iniciais[1]/n_distances)
hyp_betas[2:length(betas_iniciais)] <- hyp_betas[2:length(betas_iniciais)] + (betas_iniciais[1]/n_distances)/(n_treatment-1)
betas[[i]] <- hyp_betas
dists[[i]] <- dist(rbind(betas_iniciais, hyp_betas), method = "euclidean")
}
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
# lista para armazenar os conjuntos de dados
datasets <- list()
switch(distribution,
"normal" = {
link <- c("identity")
variance <- c("constant")
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
mu <- X%*%betas[[j]]
y <- rnorm(sample_size, mean = mu, sd = 1)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"poisson" = {
link <- c("log")
variance <- c("tweedie")
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
lambda <- exp(X%*%betas[[j]])
y <- rpois(sample_size,
lambda = lambda)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"binomial" = {
link <-  "logit"
variance  <-  "binomialP"
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
p <- exp(X%*%betas[[j]])/(1 + exp(X%*%betas[[j]]))
y <- rbinom(sample_size,
p = p,
size = 10)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
},
"beta" = {
link <- "logit"
variance <- "binomialP"
for (j in 1:length(betas)) {
datasets[[j]] <- list()
length(datasets[[j]]) <- n_datasets
for (i in 1:n_datasets) {
p <- exp(X%*%betas[[j]])/(1 + exp(X%*%betas[[j]]))
y <- gamlss.dist::rBE(sample_size,
mu = p,
sigma = 0.2)
datasets[[j]][[i]] <- data.frame(y = y,
x = trat)
}
}
}
)
switch(distribution,
"binomial" = {form = y/10~x},
{form = y~x}
)
# preditor matricial
Z0 <- mc_id(datasets[[1]][[1]]) # matriz identidade para o preditor matricial
models <- list()
switch(distribution,
"binomial" = {
for (j in 1:length(betas)) {
models[[j]] <- list()
length(models[[j]]) <- n_datasets
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
Ntrial = list(10),
data = datasets[[j]][[i]])
models[[j]][[i]] <- fit
print(paste(j,i))
}
}
},
{
for (j in 1:length(betas)) {
models[[j]] <- list()
length(models[[j]]) <- n_datasets
for (i in 1:n_datasets) {
fit <-
mcglm(linear_pred = c(form),
matrix_pred = list(c(Z0)),
link = link,
variance = variance,
data = datasets[[j]][[i]])
models[[j]][[i]] <- fit
print(paste(j,i))
}
}
}
)
betas[[1]]
models[[1]]
betas[[2]]
models[[2]]
betas[[3]]
models[[3]]
betas[[4]]
models[[4]]
betas[[5]]
models[[5]]
